{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ai_Q&A.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPOALHMQgxMw"
      },
      "source": [
        "!pip3 install en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndc2m_BPg2UO"
      },
      "source": [
        "!python3 -m spacy download en_core_web_sm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCevygUag2Jy"
      },
      "source": [
        "pip install wikipedia\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKVJXxF0g6No"
      },
      "source": [
        "pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKsGf0QVhFeQ"
      },
      "source": [
        "pip install torch==1.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQnRhLfbhJS5"
      },
      "source": [
        "pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7UroTyPg-4l"
      },
      "source": [
        "pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_2c6Kc1ipNz"
      },
      "source": [
        "import spacy\n",
        "import wikipedia\n",
        "import os\n",
        "from gensim.summarization.bm25 import BM25\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hAKw0MiiqPq"
      },
      "source": [
        "**Download relevant data for the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGc0Wj-RIgYT"
      },
      "source": [
        "\n",
        "\n",
        "wikidata_list=[(\"visakhapatnam\", \"Q200016\"), (\"Berlin\", \"Q64\"), (\"Bucharest\", \"Q19660\"), (\"Vijayawada\", \"Q200017\"), (\"Srinagar\", \"Q170115\")]\n",
        "\n",
        "for i, j in wikidata_list:\n",
        "  fileName = \"./text/\" + i + \".txt\"\n",
        "  if not os.path.isfile(fileName):\n",
        "      page = wikipedia.page(title=i, pageid=j)\n",
        "      f = open(fileName, \"w\")\n",
        "      f.write(page.content)\n",
        "      f.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_uYEmnkgLT"
      },
      "source": [
        "all_text=''\n",
        "for i, j in wikidata_list:\n",
        "  f = open(\"./text/\" + i + \".txt\", \"r\")\n",
        "  all_text+=f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8azThrmm3xj"
      },
      "source": [
        "#question processor\n",
        "def qp(text, nlp):\n",
        "  pos = [\"NOUN\", \"PROPN\", \"ADJ\"]\n",
        "  tokens = nlp(text)\n",
        "  return ' '.join(token.text for token in tokens if token.pos_ in pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OATQiHy6oPFS"
      },
      "source": [
        "#context retriever\n",
        "def cr(num, question, sentences, nlp):\n",
        "  numberOfResults = num\n",
        "  docs = []\n",
        "  for sent in sentences:\n",
        "    s=[token.lemma_ for token in nlp(sent)]\n",
        "    docs.append(s)\n",
        "\n",
        "  bm25 = BM25(docs)\n",
        "  q=[token.lemma_ for token in nlp(question)]\n",
        "  scores = bm25.get_scores(q)\n",
        "  results = {}\n",
        "  for index, score in enumerate(scores):\n",
        "      results[index] = score\n",
        "\n",
        "  sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: item[1], reverse=True)}\n",
        "  results_list = list(sorted_results.keys())\n",
        "  final_results = results_list if len(results_list) < numberOfResults else results_list[:numberOfResults]\n",
        "  qc = \"\"\n",
        "  for f in final_results:\n",
        "      qc = qc + \" \".join(docs[f])\n",
        "  return qc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytGOPMw0pcgO"
      },
      "source": [
        "#answer retriever\n",
        "\n",
        "def QA(question, qc, tk='distilbert-base-uncased', qaModel='distilbert-base-uncased-distilled-squad'):\n",
        "\n",
        "  BertTokenizer = DistilBertTokenizer.from_pretrained(tk, return_token_type_ids=True)\n",
        "  BertForQA = DistilBertForQuestionAnswering.from_pretrained(qaModel)\n",
        "\n",
        "  encodings = BertTokenizer.encode_plus(question, qc)\n",
        "\n",
        "  inputIds, attentionMask = encodings[\"input_ids\"], encodings[\"attention_mask\"]\n",
        "\n",
        "  scoresStart, scoresEnd = BertForQA(torch.tensor([inputIds]), attention_mask=torch.tensor([attentionMask]))\n",
        "\n",
        "  tokens = inputIds[torch.argmax(scoresStart): torch.argmax(scoresEnd) + 1]\n",
        "  answerTokens = BertTokenizer.convert_ids_to_tokens(tokens, skip_special_tokens=True)\n",
        "  return BertTokenizer.convert_tokens_to_string(answerTokens)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXf6ljRAh2nr"
      },
      "source": [
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "doc = nlp(all_text)\n",
        "sentences = [sent.string.strip() for sent in doc.sents]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjUJ2tKiJsX-"
      },
      "source": [
        "originalQuestion = \"most spoken language\"\n",
        "question=qp(originalQuestion, nlp)\n",
        "questionContext = cr(10, question=question, sentences=sentences, nlp=nlp)\n",
        "print (\"ASKED QUESTION : \" + originalQuestion)\n",
        "print (\"PROCESSING QUESTION : \" + question)\n",
        "answer = QA(originalQuestion, qc=questionContext)\n",
        "print (\"ANSWER : \" + answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JttTqo4LMaqy"
      },
      "source": [
        "originalQuestion = \"what is the Capital of Romania?\"\n",
        "question=qp(originalQuestion, nlp)\n",
        "questionContext = cr(1, question=question, sentences=sentences, nlp=nlp)\n",
        "print (\"ASKED QUESTION : \" + originalQuestion)\n",
        "print (\"PROCESSING QUESTION : \" + question)\n",
        "answer = QA(originalQuestion, qc=questionContext)\n",
        "print (\"ANSWER : \" + answer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}